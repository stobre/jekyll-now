<section class="entry-content ng-binding ng-scope" ng-bind-html="postContentTrustedHtml" ui-copyright-check="" data-entry-url="/p/19630762" data-author-name="肖智博" ng-if="ownPost(post) || !isCensoring"><ul><li>收集数据<br></li><ul><li>数据集。如果是已经被人做成数据集了，这就省去了很多麻烦事</li><li>抓取。这个是 Python 做得最好的事情，优秀的包有很多，比如 <a href="https://link.zhihu.com/?target=http%3A//scrapy.org" class=" wrap external" target="_blank" rel="nofollow noreferrer">scrapy<i class="icon-external"></i></a>，<a href="https://link.zhihu.com/?target=http%3A//www.crummy.com/software/BeautifulSoup/" class=" wrap external" target="_blank" rel="nofollow noreferrer">beautifulsoup<i class="icon-external"></i></a> 等等。</li></ul><li>预处理（对<a href="https://link.zhihu.com/?target=https%3A//lists.cs.princeton.edu/pipermail/topic-models/2013-November/002593.html" class=" wrap external" target="_blank" rel="nofollow noreferrer">这里<i class="icon-external"></i></a>的高质量讨论结果的修改，下面的顺序仅限<b>英文</b>）</li></ul><ol><li>去掉抓来的数据中不需要的部分，比如 HTML TAG，只保留文本。结合 beautifulsoup 和正则表达式就可以了。<a href="https://link.zhihu.com/?target=http%3A//www.clips.ua.ac.be/pages/pattern-web" class=" wrap external" target="_blank" rel="nofollow noreferrer">pattern.web<i class="icon-external"></i></a> 也有相关功能。</li><li>处理编码问题。没错，即使是英文也需要处理编码问题！由于 Python2 的历史原因，不得不在编程的时候自己处理。英文也存在 unicode 和 utf-8 转换的问题，中文以及其他语言就更不用提了。<a href="https://link.zhihu.com/?target=http%3A//www2.imm.dtu.dk/pubdb/views/edoc_download.php/6095/pdf" class=" wrap external" target="_blank" rel="nofollow noreferrer">这里<i class="icon-external"></i></a>有一个讨论，可以参考，当然网上也有很多方案，找到一个适用于自己的最好。</li><li>将文档分割成句子。</li><li>将句子分割成词。专业的叫法是 tokenize。</li><li>拼写错误纠正。<a href="https://link.zhihu.com/?target=http%3A//pythonhosted.org/pyenchant/" class=" wrap external" target="_blank" rel="nofollow noreferrer">pyenchant<i class="icon-external"></i></a> 可以帮你！（中文就没有这么些破事！）</li><li>POS Tagging。nltk 是不二选择，还可以使用 <a href="https://link.zhihu.com/?target=http%3A//www.clips.ua.ac.be/pages/pattern" class=" wrap external" target="_blank" rel="nofollow noreferrer">pattern<i class="icon-external"></i></a>。</li><li>去掉标点符号。使用正则表达式就可以。</li><li>去掉长度过小的单词。len&lt;3 的是通常选择。</li><li>去掉 non-alpha 词。同样，可以用正则表达式完成 \W 就可以。</li><li>转换成小写。</li><li>去掉停用词。Matthew L. Jockers 提供了一份比机器学习和自然语言处理中常用的停词表<a href="https://link.zhihu.com/?target=http%3A//www.matthewjockers.net/wp-content/uploads/2013/04/uwm-workshop.zip" class=" wrap external" target="_blank" rel="nofollow noreferrer">更长的停词表<i class="icon-external"></i></a>。<a href="https://link.zhihu.com/?target=http%3A//www.datatang.com/data/43894" class=" wrap external" target="_blank" rel="nofollow noreferrer">中文的停词表<i class="icon-external"></i></a> 可以参考这个。</li><li>lemmatization/stemming。nltk 里面提供了好多种方式，推荐用 wordnet 的方式，这样不会出现把词过分精简，导致词丢掉原型的结果，如果实在不行，也用 snowball 吧，别用 porter，porter 的结果我个人太难接受了，弄出结果之后都根本不知道是啥词了。<a href="https://link.zhihu.com/?target=http%3A//www.clips.ua.ac.be/pages/MBSP" class=" wrap external" target="_blank" rel="nofollow noreferrer">MBSP <i class="icon-external"></i></a>也有相关功能。</li><li>重新去掉长度过小的词。是的，再来一遍。</li><li>重新去停词。上面这两部完全是为了更干净。</li><li>到这里拿到的基本上是非常干净的文本了。如果还有进一步需求，还可以根据 POS 的结果继续选择某一种或者几种词性的词。</li></ol><br><ul><li><a href="https://link.zhihu.com/?target=http%3A//weibo.com/1657470871/Aicj2oP8G" class=" wrap external" target="_blank" rel="nofollow noreferrer">Bag-of-Words<i class="icon-external"></i></a>! nltk 和 <a href="https://link.zhihu.com/?target=http%3A//scikit-learn.org/stable/" class=" wrap external" target="_blank" rel="nofollow noreferrer">scikit.learn<i class="icon-external"></i></a> 里面都有很完整的方案，自己选择合适的就好。这里<a href="https://link.zhihu.com/?target=http%3A//scikit-learn.org/stable/modules/feature_extraction.html%23limitations-of-the-bag-of-words-representation" class=" wrap external" target="_blank" rel="nofollow noreferrer">如果不喜欢没有次序的 unigram 模型<i class="icon-external"></i></a>，可以自行选择 bi-gram 和 tri-gram 以及更高的 n-gram 模型。nltk 和 sklearn里面都有相关的处理方法。</li><li>更高级的特征。</li><ul><li>TF-IDF。这个 nltk 和 sklearn 里面也都有。</li><li><a href="https://link.zhihu.com/?target=http%3A//scikit-learn.org/stable/modules/feature_extraction.html%23vectorizing-a-large-text-corpus-with-the-hashing-trick" class=" wrap external" target="_blank" rel="nofollow noreferrer">Hashing<i class="icon-external"></i></a>！</li></ul><li>训练模型</li><ul><li>到这里，就根据自己的应用选择合适的学习器就好了。</li><li>分类，情感分析等。<a href="https://link.zhihu.com/?target=http%3A//scikit-learn.org/stable/supervised_learning.html" class=" wrap external" target="_blank" rel="nofollow noreferrer">sklearn<i class="icon-external"></i></a> 里面很多方法，pattern 里有<a href="https://link.zhihu.com/?target=http%3A//www.clips.ua.ac.be/pages/pattern-en%23sentiment" class=" wrap external" target="_blank" rel="nofollow noreferrer">情感分析的模块<i class="icon-external"></i></a>，nltk 中也有一些分类器。</li><li>主题发现</li><ul><li><a href="https://link.zhihu.com/?target=http%3A//scikit-learn.org/stable/auto_examples/applications/topics_extraction_with_nmf.html%23example-applications-topics-extraction-with-nmf-py" class=" wrap external" target="_blank" rel="nofollow noreferrer">NMF<i class="icon-external"></i></a></li><li><a href="https://link.zhihu.com/?target=http%3A//radimrehurek.com/gensim/wiki.html%23latent-dirichlet-allocation" class=" wrap external" target="_blank" rel="nofollow noreferrer">(Online) Latent Dirichlet Allocation<i class="icon-external"></i></a></li><li><a href="https://link.zhihu.com/?target=http%3A//radimrehurek.com/gensim/models/word2vec.html" class=" wrap external" target="_blank" rel="nofollow noreferrer">word2vec<i class="icon-external"></i></a></li></ul><li>自动文摘。这个自己写吧，没发现什么成型的工具。</li></ul><li>Draw results</li><ul><li>Matplotlib</li><li>Tag cloud</li><li><a href="https://link.zhihu.com/?target=http%3A//www.clips.ua.ac.be/pages/pattern-graph" class=" wrap external" target="_blank" rel="nofollow noreferrer">Graph<i class="icon-external"></i></a></li></ul></ul><br><p><b>----------更新分割线 2013.12.06 --------------</b></p>说明：在预处理部分仅仅针对英文的情况，由于中英文在分词上是不同的机制，所以在处理中文的时候需要根据情况进行，个人经验是在去停词之前分词。这部分有待完善。<ul><li>中文分词。<a href="https://link.zhihu.com/?target=https%3A//github.com/fxsjy/jieba" class=" wrap external" target="_blank" rel="nofollow noreferrer">jieba<i class="icon-external"></i></a> 或者 <a href="https://link.zhihu.com/?target=http%3A//nlp.stanford.edu/software/segmenter.shtml" class=" wrap external" target="_blank" rel="nofollow noreferrer">Stanford (Chinese) Word Segmenter<i class="icon-external"></i></a>。jieba 是纯 Python 写的，Stanford 的可以通过 <a href="https://link.zhihu.com/?target=http%3A//nltk.org/" class=" wrap external" target="_blank" rel="nofollow noreferrer">nltk<i class="icon-external"></i></a> 调用，<a href="https://link.zhihu.com/?target=http%3A//code.google.com/p/fudannlp/" class=" wrap external" target="_blank" rel="nofollow noreferrer">复旦 NLP<i class="icon-external"></i></a> 也可以用 Python 调用。</li></ul><p>-------------------------------------------------------------------------------------<br>如果你觉得我的答案对你有帮助，可以考虑向我付费：</p><img src="https://pic3.zhimg.com/336c9d48a945e143c4ec5afb415435fa_b.jpg" data-rawwidth="256" data-rawheight="256" class="content_image" width="256"></section>
